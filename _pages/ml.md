---
layout: single
permalink: /ml/
author_profile: true
classes: wide
---

Cette section reprend divers projets, etudes, MOOCs que je mène en machine learning.

## Culture générale des algorithmes
[Segmentation des datasets, métriques de performance](https://alexpeterbec.github.io/metrics/scoring/algorithm-scoring/)

## Tensorflow
[De zéro à Tensorflow](https://alexpeterbec.github.io/definitions/tensorflow/tensors/tensorflow-theorie/)

![image](/assets/images/banners/deep-learning1.jpg){: .align-center }

## Concepts du deep learning, descente de gradient

[Qu'est-ce qu'un réseau de neurones ?](https://alexpeterbec.github.io/nn/nn-intro-dl/)

[Réseau de neurones, Problème de classification binaire](https://alexpeterbec.github.io/nn/logreg/nn-log-reg/)

[Principe de la descente de gradient et des graphes de calcul](https://alexpeterbec.github.io/ml/graph/algebre/nn-gradient-computation/)

[Descente de gradient appliquée à la regression logistique](https://alexpeterbec.github.io/ml/nn-gradient-applied/)

[Python : La vectorisation](https://alexpeterbec.github.io/ml/python/nn-vectorization/)

## Réseaux de neurones à une couche cachée

[Assemblages de neurones](https://alexpeterbec.github.io/neural/nets/shallow-network/)

[Fonctions d'activation](https://alexpeterbec.github.io/deep/learning/activation/shallow-activation-functions/)

[Descente de gradient (avec une couche cachée)](https://alexpeterbec.github.io/deep/learning/gradient/shallow-gradient-descent/)

[Initialisation des poids](https://alexpeterbec.github.io/deep/learning/weights/shallow-weight-init/)

## Réseaux de neurones profonds

[Assemblage de couches](https://alexpeterbec.github.io/deep-learning/deep-networks/) : Notations, forward prop, dimensions des matrices.

[Deep Learning](https://alexpeterbec.github.io/deep-learning/deep-representation/) : Intérêt des réseaux profonds, schemas, Hyper-paramètres.

![image](/assets/images/banners/deep-learning2.jpg){: .align-center }

## Aspects pratiques du deep learning

Mise en place du deep learning

Régularisation du réseau

Les problèmes de l'optimisation

## Algorithmes d'optimisation

Descente de gradient mini-batch

Pondération exponentielle (EWMA)

Autres méthodes : Descente de gradient dynamique, RMSprop, Adam Optimizer, Adaptation du pas d'apprentissage, Optima locaux

## Tuning des paramètres, Batch normalisation, Tensorflow

Tuning des hyper-paramètres

Batch-normalisation

Classification multiclasse - Softmax

Introduction à Tensorflow


![image](/assets/images/banners/deep-learning3.jpg){: .align-center }

## Première partie

Orthogonalisation, Métriques d'évaluation, Répartition train/test, Performances

## Seconde partie

Nettoyage des données, Transfer learning, Multi-task learning, end-to-end deep learning

![image](/assets/images/banners/deep-learning4.jpg){: .align-center }

## Les bases des CNN

## Etudes de cas

## Detection d'objets

## Reconnaissance de visages & Neural style transfer

![image](/assets/images/banners/deep-learning5.jpg){: .align-center }

## Réseaux récurrents

## NLP & Word-embedding

## Mécanismes d'attention
